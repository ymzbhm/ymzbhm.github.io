<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>N-gram | 有梦者不毁灭</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="在自然语言处理中，N-Gram模型是一种用于建模词序列中词与词之间关系的统计模型。它基于前一个（或前几个）词的信息来预测当前词的概率。N-Gram的核心思想是：给定一个词序列，预测下一个词的概率只依赖于前面的N-1个词。 N-Gram的定义： Unigram（1-Gram）：每个词的概率是独立的，只与该词本身有关。 Bigram（2-Gram）：当前词的概率依赖于前一个词。 Trigram（3-G">
<meta property="og:type" content="article">
<meta property="og:title" content="N-gram">
<meta property="og:url" content="http://example.com/2024/11/21/N-gram/index.html">
<meta property="og:site_name" content="有梦者不毁灭">
<meta property="og:description" content="在自然语言处理中，N-Gram模型是一种用于建模词序列中词与词之间关系的统计模型。它基于前一个（或前几个）词的信息来预测当前词的概率。N-Gram的核心思想是：给定一个词序列，预测下一个词的概率只依赖于前面的N-1个词。 N-Gram的定义： Unigram（1-Gram）：每个词的概率是独立的，只与该词本身有关。 Bigram（2-Gram）：当前词的概率依赖于前一个词。 Trigram（3-G">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-11-21T12:19:36.000Z">
<meta property="article:modified_time" content="2024-11-21T12:24:59.930Z">
<meta property="article:author" content="ymzbhm">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="有梦者不毁灭" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">有梦者不毁灭</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-N-gram" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/11/21/N-gram/" class="article-date">
  <time class="dt-published" datetime="2024-11-21T12:19:36.000Z" itemprop="datePublished">2024-11-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      N-gram
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在自然语言处理中，<strong>N-Gram模型</strong>是一种用于建模词序列中词与词之间关系的统计模型。它基于前一个（或前几个）词的信息来预测当前词的概率。N-Gram的核心思想是：给定一个词序列，预测下一个词的概率只依赖于前面的N-1个词。</p>
<h3 id="N-Gram的定义："><a href="#N-Gram的定义：" class="headerlink" title="N-Gram的定义："></a>N-Gram的定义：</h3><ul>
<li><strong>Unigram（1-Gram）</strong>：每个词的概率是独立的，只与该词本身有关。</li>
<li><strong>Bigram（2-Gram）</strong>：当前词的概率依赖于前一个词。</li>
<li><strong>Trigram（3-Gram）</strong>：当前词的概率依赖于前两个词。</li>
<li><strong>N-Gram（N&gt;3）</strong>：当前词的概率依赖于前N-1个词。</li>
</ul>
<p>对于一个给定的词序列 w1,w2,w3,…,wTw_1, w_2, w_3, \dots, w_T，N-Gram模型计算词序列的联合概率可以写作：<br>$$<br>P(w_1, w_2, w_3, \dots, w_T) &#x3D; P(w_1) P(w_2 | w_1) P(w_3 | w_1, w_2) \dots P(w_T | w_1, w_2, \dots, w_{T-1})<br>$$</p>
<h3 id="1-计算-N-Gram-概率"><a href="#1-计算-N-Gram-概率" class="headerlink" title="1. 计算 N-Gram 概率"></a>1. <strong>计算 N-Gram 概率</strong></h3><p>N-Gram模型的关键是 <strong>条件概率</strong>，即给定前N-1个词的情况下，计算当前词出现的概率。具体地，对于一个 <strong>N-Gram模型</strong>，条件概率可以写成：<br>$$<br>P(w_t | w_{t-1}, w_{t-2}, \dots, w_{t-N+1}) &#x3D; \frac{\text{count}(w_{t-N+1}, \dots, w_t)}{\text{count}(w_{t-N+1}, \dots, w_{t-1})}<br>$$<br>其中：</p>
<ul>
<li>**count(w_{t-N+1}, …, w_t)**：表示N-Gram中词序列 wt−N+1,…,wtw_{t-N+1}, \dots, w_t 在训练语料中出现的次数。</li>
<li>**count(w_{t-N+1}, …, w_{t-1})**：表示前N-1个词 wt−N+1,…,wt−1w_{t-N+1}, \dots, w_{t-1} 出现的次数。</li>
</ul>
<h3 id="2-概率估计：简化版"><a href="#2-概率估计：简化版" class="headerlink" title="2. 概率估计：简化版"></a>2. <strong>概率估计：简化版</strong></h3><p>对于一个特定的N-Gram模型，计算每个词的条件概率时，通常是基于训练语料中的频率估计。具体计算方法如下：</p>
<h4 id="Unigram（1-Gram）："><a href="#Unigram（1-Gram）：" class="headerlink" title="Unigram（1-Gram）："></a><strong>Unigram（1-Gram）</strong>：</h4><p>在1-Gram模型中，当前词的概率仅与词本身有关，可以通过该词在语料中的频率来估计：<br>$$<br>P(w_t) &#x3D; \frac{\text{count}(w_t)}{\text{count(all words)}}<br>$$<br>其中：</p>
<ul>
<li>**count(w_t)**：是词 wtw_t 出现在训练语料中的次数。</li>
<li>**count(all words)**：是语料库中所有词的总数。</li>
</ul>
<h4 id="Bigram（2-Gram）："><a href="#Bigram（2-Gram）：" class="headerlink" title="Bigram（2-Gram）："></a><strong>Bigram（2-Gram）</strong>：</h4><p>在2-Gram模型中，当前词的概率依赖于前一个词，可以用条件概率来表示：<br>$$<br>P(w_t | w_{t-1}) &#x3D; \frac{\text{count}(w_{t-1}, w_t)}{\text{count}(w_{t-1})}<br>$$<br>其中：</p>
<ul>
<li>**count(w_{t-1}, w_t)**：是词对 wt−1,wtw_{t-1}, w_t 出现在语料中的次数。</li>
<li>**count(w_{t-1})**：是词 wt−1w_{t-1} 出现在语料中的次数。</li>
</ul>
<h4 id="Trigram（3-Gram）："><a href="#Trigram（3-Gram）：" class="headerlink" title="Trigram（3-Gram）："></a><strong>Trigram（3-Gram）</strong>：</h4><p>在3-Gram模型中，当前词的概率依赖于前两个词，可以用条件概率来表示：<br>$$<br>P(w_t | w_{t-2}, w_{t-1}) &#x3D; \frac{\text{count}(w_{t-2}, w_{t-1}, w_t)}{\text{count}(w_{t-2}, w_{t-1})}<br>$$<br>其中：</p>
<ul>
<li>**count(w_{t-2}, w_{t-1}, w_t)**：是三元组 wt−2,wt−1,wtw_{t-2}, w_{t-1}, w_t 出现在语料中的次数。</li>
<li>**count(w_{t-2}, w_{t-1})**：是二元组 wt−2,wt−1w_{t-2}, w_{t-1} 出现的次数。</li>
</ul>
<h3 id="3-平滑（Smoothing）"><a href="#3-平滑（Smoothing）" class="headerlink" title="3. 平滑（Smoothing）"></a>3. <strong>平滑（Smoothing）</strong></h3><p>在实际计算时，由于某些N-Grams在训练语料中可能从未出现过（即频率为零），这会导致在计算概率时出现为零的情况。为了避免这种情况，我们需要使用 <strong>平滑技术</strong>。常见的平滑方法有：</p>
<ul>
<li><p><strong>加一平滑（Additive Smoothing）</strong>： 对于每个N-Gram，假设它的频率最小为1，这样可以避免出现零概率。加一平滑的方法如下：<br>$$<br>P(w_t | w_{t-1}, \dots, w_{t-N+1}) &#x3D; \frac{\text{count}(w_{t-N+1}, \dots, w_t) + 1}{\text{count}(w_{t-N+1}, \dots, w_{t-1}) + V}<br>$$<br>其中：</p>
<ul>
<li>VV 是词汇表的大小（即训练语料中不同词的数量）。</li>
<li>这种方法确保了即使某些N-Grams没有出现在训练数据中，它们的概率也不会是零。</li>
</ul>
</li>
<li><p><strong>Kneser-Ney平滑</strong>：这是另一种更复杂的平滑方法，主要用于改善低频N-Grams的概率估计。它基于词尾出现的次数进行平滑，常用于语言建模。</p>
</li>
</ul>
<h3 id="4-N-Gram模型的训练与评估"><a href="#4-N-Gram模型的训练与评估" class="headerlink" title="4. N-Gram模型的训练与评估"></a>4. <strong>N-Gram模型的训练与评估</strong></h3><ul>
<li><strong>训练</strong>：通过遍历训练数据，统计每个N-Gram的出现次数，进而估计条件概率。</li>
<li><strong>评估</strong>：通常使用困惑度（Perplexity）来评估N-Gram模型的性能。困惑度越低，说明模型对测试数据的预测能力越强。困惑度的计算公式为：</li>
</ul>
<p>$$<br>{Perplexity} &#x3D; \exp \left( -\frac{1}{T} \sum_{t&#x3D;1}^{T} \log P(w_t | w_{t-1}, w_{t-2}, \dots, w_{t-N+1}) \right)<br>$$</p>
<p>其中：</p>
<ul>
<li><p>TT 是测试数据中的总词数。</p>
</li>
<li><p>$$<br>P(w_t | w_{t-1}, w_{t-2}, \dots, w_{t-N+1})<br>$$</p>
<p> 是模型预测的条件概率。</p>
</li>
</ul>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><ul>
<li><strong>N-Gram模型</strong>是基于前N-1个词来预测当前词的概率，计算时基于词频统计。</li>
<li><strong>条件概率</strong>的计算依赖于N-Gram的出现频率，并通过平滑技术解决低频或未出现的N-Gram问题。</li>
<li>常见的N-Gram模型包括 <strong>Unigram、Bigram、Trigram</strong> 等，应用广泛，但在处理长距离依赖时效果有限，因此往往与深度学习模型（如RNN、Transformer）结合使用。</li>
</ul>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>N&#x3D;3</p>
<p>p(w1,w2,w3…wn)&#x3D;p(w1)p(w2| w1)P(w3|w1w2)p(w4|w2w3)……p(wn | wn-2 wn-1)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/11/21/N-gram/" data-id="cm3rac8dv0000r0tsfy8uegj2" data-title="N-gram" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/10/25/%E4%B9%B0%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BA/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">买股票的最佳时机</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/LeetCode-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" rel="tag">LeetCode-动态规划</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/LeetCode-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 20px;">LeetCode-动态规划</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">深度学习</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">十一月 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">十月 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/11/21/N-gram/">N-gram</a>
          </li>
        
          <li>
            <a href="/2024/10/25/%E4%B9%B0%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BA/">买股票的最佳时机</a>
          </li>
        
          <li>
            <a href="/2024/10/25/%E6%9C%80%E5%A4%A7%E5%AD%90%E6%95%B0%E7%BB%84%E5%92%8C/">最大子数组和</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 ymzbhm<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>